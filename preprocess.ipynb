{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "fp_train = r\"data/train.csv\"\n",
    "fp_test = r\"data/test.txt\"\n",
    "fp_valid = r\"data/valid.csv\"\n",
    "fp_dict = r\"data/dict.txt\"\n",
    "\n",
    "data_train = pd.read_csv(fp_train, names=[\"谜底\", \"谜面\"]).to_dict(orient=\"records\")\n",
    "data_valid = pd.read_csv(fp_valid, names=[\"谜底\", \"谜面\"]).to_dict(orient=\"records\")\n",
    "\n",
    "data_dict = open(fp_dict, \"r\").read().strip().split(\"\\n\")\n",
    "data_test = open(fp_test, \"r\").read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "fp_zi_dataset = r\"resource/zi-dataset.tsv\"\n",
    "fp_stroke = r\"resource/stroke.csv\"\n",
    "fp_xinhua_word = r\"resource/word.json\"\n",
    "\n",
    "data_zi_dataset = pd.read_csv(fp_zi_dataset, sep=\"\\t\").to_dict(orient=\"records\")\n",
    "data_zi_dataset = {item[\"zi\"]:item for item in data_zi_dataset}\n",
    "# print(len(data_zi_dataset)) 无重复\n",
    "\n",
    "data_xinhua_word = pd.read_json(fp_xinhua_word).to_dict(orient=\"records\")\n",
    "# data_xinhua_word = {item[\"word\"]:item for item in data_xinhua_word}\n",
    "# print(len(data_xinhua_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4371 16631\n",
      "1457 5480\n",
      "1458\n",
      "7286\n"
     ]
    }
   ],
   "source": [
    "characters_train = list(set([item[\"谜底\"] for item in data_train]))\n",
    "print(len(characters_train), len(data_train))\n",
    "characters_valid = list(set([item[\"谜底\"] for item in data_valid]))\n",
    "print(len(characters_valid), len(data_valid))\n",
    "characters_test = list(set(data_dict))\n",
    "print(len(characters_test))\n",
    "characters_all = list(set(characters_train+characters_valid+characters_test))\n",
    "print(len(characters_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肀\n",
      "王\n",
      "《\n",
      "_\n",
      "`\n",
      "(\n",
      "<\n",
      "范\n",
      "♂\n",
      "﻿有\n",
      "7276\n"
     ]
    }
   ],
   "source": [
    "from pypinyin import pinyin as py\n",
    "\n",
    "# 从zi-dataset里获取基本信息\n",
    "character_info = dict()\n",
    "for character in characters_all:\n",
    "    if character in data_zi_dataset:\n",
    "        item = data_zi_dataset[character]\n",
    "        stroke_count = item[\"stroke_count\"].replace(\"画\", \"\")\n",
    "        if not isinstance(item[\"mandarin_pinyin\"], float):\n",
    "            pinyin = item[\"mandarin_pinyin\"].replace(\", \", \"、\")\n",
    "        else:\n",
    "            pinyin = py(character)[0][0]\n",
    "        radical = item[\"radical\"].replace(\"/\", \"\")\n",
    "        radical_count = item[\"radical_stroke_count\"]\n",
    "        radical_pinyin = item[\"radical_pinyin\"].replace(\", \", \"、\")\n",
    "        components = item[\"leaf_component\"].replace(\"/\", \"\") if not isinstance(item[\"leaf_component\"], float) else character\n",
    "    else:\n",
    "        print(character)\n",
    "        continue\n",
    "\n",
    "    character_info[character] = {\n",
    "        \"汉字\": character,\n",
    "        \"笔画数\": stroke_count,\n",
    "        \"拼音\": pinyin,\n",
    "        \"部首\": radical,\n",
    "        \"部首的笔画数\": radical_count,\n",
    "        \"部首的拼音\": radical_pinyin,\n",
    "        \"组件\": components,\n",
    "    }\n",
    "\n",
    "print(len(character_info))\n",
    "\n",
    "# 下面三个字只出现在了新华字典中，没有出现在zi-dataset中\n",
    "character_info[\"范\"] = {\n",
    "    \"汉字\": \"范\",\n",
    "    \"笔画数\": \"8\",\n",
    "    \"拼音\": \"fàn\",\n",
    "    \"部首\": \"艹\",\n",
    "    \"部首的笔画数\": \"3\",\n",
    "    \"部首的拼音\": \"cǎo\",\n",
    "    \"组件\": \"艹氾\",\n",
    "}\n",
    "character_info[\"肀\"] = {\n",
    "    \"汉字\": \"肀\",\n",
    "    \"笔画数\": \"4\",\n",
    "    \"拼音\": \"yù\",\n",
    "    \"部首\": \"肀\",\n",
    "    \"部首的笔画数\": \"4\",\n",
    "    \"部首的拼音\": \"肀\",\n",
    "    \"组件\": \"肀\",\n",
    "}\n",
    "character_info[\"王\"] = {\n",
    "    \"汉字\": \"王\",\n",
    "    \"笔画数\": \"4\",\n",
    "    \"拼音\": \"wáng\",\n",
    "    \"部首\": \"王\",\n",
    "    \"部首的笔画数\": \"4\",\n",
    "    \"部首的拼音\": \"wáng\",\n",
    "    \"组件\": \"王\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从新华字典里获取释义信息\n",
    "for character in character_info:\n",
    "    explanations = [i[\"explanation\"] for i in data_xinhua_word if i[\"word\"] == character]\n",
    "    if len(explanations) == 0:\n",
    "        character_info[character][\"释义\"] = \"一个罕见的字\"\n",
    "    else:\n",
    "        character_info[character][\"释义\"] = \" \".join([i.replace(\"\\n\", \"\") for i in explanations])\n",
    "    text = \"[SEP]\".join([f\"{k}是{v}\" for k, v in character_info[character].items() if k != \"描述\"])\n",
    "    character_info[character][\"描述\"] = text\n",
    "    # print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fp_character_information = r\"resource/character_information.json\"\n",
    "json.dump(character_info, open(fp_character_information, \"w\"), indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_train = list()\n",
    "st_valid = list()\n",
    "\n",
    "\n",
    "for item in data_train:\n",
    "    if item[\"谜底\"] in character_info:\n",
    "        item[\"谜底_描述\"] = character_info[item[\"谜底\"]][\"描述\"]\n",
    "        item[\"label\"] = 1\n",
    "        st_train.append(item)\n",
    "for item in data_valid:\n",
    "    if item[\"谜底\"] in character_info:\n",
    "        item[\"谜底_描述\"] = character_info[item[\"谜底\"]][\"描述\"]\n",
    "        item[\"label\"] = 1\n",
    "        st_valid.append(item)\n",
    "        \n",
    "\n",
    "pd.DataFrame(st_train).to_csv(\"data/st_train.csv\", index=False)\n",
    "pd.DataFrame(st_valid).to_csv(\"data/st_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "st_train_enhancement = list()\n",
    "\n",
    "st_train = pd.read_csv(r\"data/st_train.csv\").to_dict(orient=\"records\")\n",
    "\n",
    "for item in st_train:\n",
    "    candidate1 = random.sample(st_train, 1)\n",
    "    while candidate1[0][\"谜底\"] == item[\"谜底\"]:\n",
    "        candidate1 = random.sample(st_train, 1)\n",
    "    candidate1 = copy.deepcopy(candidate1[0])\n",
    "    candidate1[\"谜底\"] = item[\"谜底\"]\n",
    "    candidate1[\"谜底_描述\"] = item[\"谜底_描述\"]\n",
    "    candidate1[\"label\"] = 0\n",
    "    \n",
    "    candidate2 = random.sample(st_train, 1)\n",
    "    while candidate2[0][\"谜面\"] == item[\"谜面\"]:\n",
    "        candidate2 = random.sample(st_train, 1)\n",
    "    candidate2 = copy.deepcopy(candidate2[0])\n",
    "    candidate2[\"谜面\"] = item[\"谜面\"]\n",
    "    candidate2[\"label\"] = 0\n",
    "\n",
    "    # print(item)\n",
    "    # print(candidate1)\n",
    "    # print(candidate2)\n",
    "    st_train_enhancement.append(item)\n",
    "    st_train_enhancement.append(candidate1)\n",
    "    st_train_enhancement.append(candidate2)\n",
    "\n",
    "pd.DataFrame(st_train_enhancement).to_csv(\"data/st_train_enhancement.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16631 16626 5480 5480\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train), len(st_train), len(data_valid), len(st_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "fp_test = r\"data/test.txt\"\n",
    "fp_preds_riddle = r\"/Users/nascent/Documents/Course_22spring/万小军语义计算与知识检索/hw3/output/hfl-chinese-roberta-wwm-ext_train_enhancement_epochs-3_batchsize-16_2022-06-06_00-24-02/test_predictions_with_riddle.txt\"\n",
    "fp_preds = r\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6d709b957fa7e41ecd6dffde8bd8771003123d90276a71628c7b828d5dbb053"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('space2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
