{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "st_train = pd.read_csv(\"/home/featurize/data/data/st_train.csv\").to_dict(orient=\"records\")\n",
    "st_valid = pd.read_csv(\"/home/featurize/data/data/st_valid.csv\").to_dict(orient=\"records\")\n",
    "st_train_enhancement = pd.read_csv(\"/home/featurize/data/data/st_train_enhancement.csv\").to_dict(orient=\"records\")\n",
    "\n",
    "data_test = open(\"/home/featurize/data/data/test.txt\", \"r\").read().strip().split(\"\\n\")\n",
    "dict_test = open(\"/home/featurize/data/data/dict.txt\", \"r\").read().strip().split(\"\\n\")\n",
    "character_info = json.load(open(\"/home/featurize/data/data/character_information.json\", \"r\"))\n",
    "\n",
    "dict_valid = list(set([item[\"谜底\"] for item in st_valid]))\n",
    "\n",
    "# 调整备选词表，去除无意义的词（实际上删除了一个点号和一个书名号）\n",
    "for i in range(len(dict_test)):\n",
    "    if dict_test[i] == '\\ufeff有':\n",
    "        dict_test[i] = \"有\"\n",
    "        break\n",
    "dict_test = [i for i in dict_test if i in character_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "try:\n",
    "    model_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "except:\n",
    "    word_embedding_model = models.Transformer(\"models/model_roberta_st_init\")\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model_roberta_st = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "# model_roberta_st.save(\"models/model_roberta_st_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "train_pattern = [[st_train, 200, \"train\"], [st_train, len(st_train), \"train\"], [st_train_enhancement, len(st_train_enhancement), \"train_enhancement\"]]\n",
    "train_pattern_sel = 2\n",
    "train_size = train_pattern[train_pattern_sel][1]\n",
    "st_train_sel = random.sample(train_pattern[train_pattern_sel][0], train_size)\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "evaluation_steps = train_size / 10\n",
    "\n",
    "train_examples = [InputExample(texts=[item[\"谜面\"], item[\"谜底_描述\"]], label=float(item[\"label\"])) for item in st_train_sel]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model_roberta_st)\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator([item[\"谜面\"] for item in st_valid], [item[\"谜底_描述\"] for item in st_valid], [float(item[\"label\"]) for item in st_valid])\n",
    "\n",
    "warmup_steps = len(train_dataloader) * epochs / 10\n",
    "\n",
    "model_save_path = \"output/\" + model_name.replace(\"/\", \"-\") + \"_\" + train_pattern[train_pattern_sel][2] + \"_\" + f\"epochs-{epochs}_batchsize-{batch_size}\" + \"_\" + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "from function import get_embeddings_of_characters, get_embeddings_of_riddles\n",
    "import os\n",
    "\n",
    "def get_mrr(model, split_name, riddles, character_list, character_info):\n",
    "    ebds_of_riddles= get_embeddings_of_riddles(model, riddles)\n",
    "    ebds_of_characters = get_embeddings_of_characters(model, character_list, character_info)\n",
    "\n",
    "    hits = util.semantic_search(list(ebds_of_riddles.values()), list(ebds_of_characters.values()), top_k=5)\n",
    "    mrr1 = 0\n",
    "    mrr3 = 0\n",
    "    mrr5 = 0\n",
    "    guesses = dict()\n",
    "    for i, query in enumerate(hits):\n",
    "        riddle = riddles[i]\n",
    "        # print(riddle)\n",
    "        candidates = [character_list[ans[\"corpus_id\"]] for ans in query]\n",
    "        guesses[riddle] = candidates\n",
    "        # print(candidates)\n",
    "        if split_name == \"valid\":\n",
    "            answer = \"\"\n",
    "            for item in st_valid:\n",
    "                if item[\"谜面\"] == riddle:\n",
    "                    answer = item[\"谜底\"]\n",
    "                    break\n",
    "            guesses[riddle] = [answer] + candidates\n",
    "            # print(answer)\n",
    "            for k, candidate in enumerate(candidates):\n",
    "                if candidate == answer:\n",
    "                    if k+1 == 1:\n",
    "                        mrr1 += 1\n",
    "                    if k+1 <= 3:\n",
    "                        mrr3 += (1/float(k+1))\n",
    "                    if k+1 <= 5:\n",
    "                        mrr5 += (1/float(k+1))\n",
    "    with open(os.path.join(model_save_path, split_name + \"_predictions_with_riddle.txt\"), \"w\") as fout:\n",
    "            for k, v in guesses.items():\n",
    "                if split_name == \"valid\":\n",
    "                    fout.write(k + \"\\t\" + v[0] + \"\\t\" + \"\\t\".join(v[1:]) + \"\\n\")\n",
    "                else:\n",
    "                    fout.write(k + \"\\t\" + \"\\t\".join(v[:5]) + \"\\n\")\n",
    "    with open(os.path.join(model_save_path, split_name + \"_predictions.txt\"), \"w\") as fout:\n",
    "            for k, v in guesses.items():\n",
    "                if split_name == \"valid\":\n",
    "                    fout.write(v[0] + \"\\t\" + \"\\t\".join(v[1:]) + \"\\n\")\n",
    "                else:\n",
    "                    fout.write(\"\\t\".join(v[:5]) + \"\\n\")\n",
    "    if split_name == \"valid\":\n",
    "        with open(os.path.join(model_save_path, \"valid_metrics.txt\"), \"w\") as fout:\n",
    "            fout.write(\"mrr1: {:.4f}\\n\".format(mrr1/len(st_valid)))\n",
    "            fout.write(\"mrr3: {:.4f}\\n\".format(mrr3/len(st_valid)))\n",
    "            fout.write(\"mrr5: {:.4f}\\n\".format(mrr5/len(st_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5480/5480 [01:39<00:00, 55.19it/s]\n",
      "100%|██████████| 1457/1457 [00:29<00:00, 49.50it/s]\n"
     ]
    }
   ],
   "source": [
    "get_mrr(model_roberta_st, \"valid\", [item[\"谜面\"] for item in st_valid], list(set([item[\"谜底\"] for item in st_valid])), character_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54507c6364854af0bcade0772276a04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f098322ff73d4563bd216754c1d8da1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_roberta_st.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], \n",
    "    epochs=epochs, \n",
    "    warmup_steps=warmup_steps, \n",
    "    evaluator=evaluator, \n",
    "    evaluation_steps=evaluation_steps,\n",
    "    output_path=model_save_path,\n",
    "    save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5480/5480 [01:42<00:00, 53.21it/s]\n",
      "100%|██████████| 1457/1457 [00:28<00:00, 50.49it/s]\n",
      "100%|██████████| 5413/5413 [01:39<00:00, 54.51it/s]\n",
      "100%|██████████| 1456/1456 [00:28<00:00, 50.22it/s]\n"
     ]
    }
   ],
   "source": [
    "get_mrr(model_roberta_st, \"valid\", [item[\"谜面\"] for item in st_valid], list(set([item[\"谜底\"] for item in st_valid])), character_info)\n",
    "get_mrr(model_roberta_st, \"test\", data_test, dict_test, character_info)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b114295533213be714c497b6c7c7c36862ca698da8b4418201631177dea05d47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
